{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "%config Completer.use_jedi = False\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clementinelacey/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (18,20,21,23,24,26,27,29,31,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/anonymized_bsc_dataPVZ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sttr Term</th>\n",
       "      <th>course_#</th>\n",
       "      <th>course_title</th>\n",
       "      <th>Sttr Student Load</th>\n",
       "      <th>Sttr Stu Credits</th>\n",
       "      <th>Sttr Attempted Cred</th>\n",
       "      <th>Sttr Stu Final Grades</th>\n",
       "      <th>Sttr Cmpl Cred</th>\n",
       "      <th>Sttr Term Gpa</th>\n",
       "      <th>faculty</th>\n",
       "      <th>...</th>\n",
       "      <th>FA5 Enr</th>\n",
       "      <th>FA5 Class</th>\n",
       "      <th>FA5 Major</th>\n",
       "      <th>FA6 Enr</th>\n",
       "      <th>FA6 Class</th>\n",
       "      <th>FA6 Major</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Grad Term</th>\n",
       "      <th>Grad Year</th>\n",
       "      <th>Grad Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/SP</td>\n",
       "      <td>DA 101 A</td>\n",
       "      <td>Basic Ballet</td>\n",
       "      <td>F</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>W</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.667</td>\n",
       "      <td>Melissa Turnage</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>18/SP</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>Psychology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15/SP</td>\n",
       "      <td>HI 155 A-ES</td>\n",
       "      <td>Reforming America</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A-</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.667</td>\n",
       "      <td>Victoria Ott</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>18/SP</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>Psychology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15/SP</td>\n",
       "      <td>PL 200 A-CI</td>\n",
       "      <td>Ethical Choice</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B+</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.667</td>\n",
       "      <td>William Myers</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>18/SP</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>Psychology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15/SP</td>\n",
       "      <td>IDS 200 A-CI</td>\n",
       "      <td>Intro to Human Rights (GP)</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.667</td>\n",
       "      <td>Sandra Sprayberry</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>18/SP</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>Psychology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15/WI</td>\n",
       "      <td>GEN E299 49</td>\n",
       "      <td>Exploration Project</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Vincent Gawronski</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>18/SP</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>Psychology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sttr Term      course_#                course_title Sttr Student Load  \\\n",
       "0     15/SP      DA 101 A                Basic Ballet                 F   \n",
       "1     15/SP   HI 155 A-ES           Reforming America                 F   \n",
       "2     15/SP   PL 200 A-CI              Ethical Choice                 F   \n",
       "3     15/SP  IDS 200 A-CI  Intro to Human Rights (GP)                 F   \n",
       "4     15/WI   GEN E299 49         Exploration Project                 F   \n",
       "\n",
       "   Sttr Stu Credits  Sttr Attempted Cred Sttr Stu Final Grades  \\\n",
       "0               0.5                  3.0                     W   \n",
       "1               1.0                  NaN                    A-   \n",
       "2               1.0                  NaN                    B+   \n",
       "3               1.0                  NaN                     A   \n",
       "4               1.0                  1.0                     S   \n",
       "\n",
       "   Sttr Cmpl Cred  Sttr Term Gpa            faculty  ...  FA5 Enr FA5 Class  \\\n",
       "0             3.0          3.667    Melissa Turnage  ...      NaN       NaN   \n",
       "1             3.0          3.667       Victoria Ott  ...      NaN       NaN   \n",
       "2             3.0          3.667      William Myers  ...      NaN       NaN   \n",
       "3             3.0          3.667  Sandra Sprayberry  ...      NaN       NaN   \n",
       "4             1.0          0.000  Vincent Gawronski  ...      NaN       NaN   \n",
       "\n",
       "  FA5 Major  FA6 Enr FA6 Class FA6 Major  Graduated Grad Term Grad Year  \\\n",
       "0       NaN      NaN       NaN       NaN          Y     18/SP    1718.0   \n",
       "1       NaN      NaN       NaN       NaN          Y     18/SP    1718.0   \n",
       "2       NaN      NaN       NaN       NaN          Y     18/SP    1718.0   \n",
       "3       NaN      NaN       NaN       NaN          Y     18/SP    1718.0   \n",
       "4       NaN      NaN       NaN       NaN          Y     18/SP    1718.0   \n",
       "\n",
       "   Grad Major  \n",
       "0  Psychology  \n",
       "1  Psychology  \n",
       "2  Psychology  \n",
       "3  Psychology  \n",
       "4  Psychology  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sttr Term</th>\n",
       "      <th>course_#</th>\n",
       "      <th>course_title</th>\n",
       "      <th>Sttr Student Load</th>\n",
       "      <th>Sttr Stu Credits</th>\n",
       "      <th>Sttr Attempted Cred</th>\n",
       "      <th>Sttr Stu Final Grades</th>\n",
       "      <th>Sttr Cmpl Cred</th>\n",
       "      <th>Sttr Term Gpa</th>\n",
       "      <th>faculty</th>\n",
       "      <th>...</th>\n",
       "      <th>FA5 Enr</th>\n",
       "      <th>FA5 Class</th>\n",
       "      <th>FA5 Major</th>\n",
       "      <th>FA6 Enr</th>\n",
       "      <th>FA6 Class</th>\n",
       "      <th>FA6 Major</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Grad Term</th>\n",
       "      <th>Grad Year</th>\n",
       "      <th>Grad Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>14/D</td>\n",
       "      <td>EH 208 B</td>\n",
       "      <td>Intermediate Writing</td>\n",
       "      <td>F</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.110</td>\n",
       "      <td>Sandra Sprayberry</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>18/SP</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>Urban Environmental Studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369</th>\n",
       "      <td>14/D</td>\n",
       "      <td>BI 115 A-SM</td>\n",
       "      <td>Organismal Biology</td>\n",
       "      <td>F</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.557</td>\n",
       "      <td>Andrew Gannon</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>14/D</td>\n",
       "      <td>PY 101 F</td>\n",
       "      <td>Introduction to Psychology</td>\n",
       "      <td>F</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C-</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.557</td>\n",
       "      <td>Gabrielle Smith</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>14/D</td>\n",
       "      <td>EH 102 C</td>\n",
       "      <td>Sem Critical Thinking Writing</td>\n",
       "      <td>F</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.557</td>\n",
       "      <td>Steven Carter</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>14/D</td>\n",
       "      <td>MS 120 D</td>\n",
       "      <td>Voice</td>\n",
       "      <td>F</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.591</td>\n",
       "      <td>Erin Ludwick</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>18/SP</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sttr Term     course_#                   course_title Sttr Student Load  \\\n",
       "6784      14/D     EH 208 B           Intermediate Writing                 F   \n",
       "7369      14/D  BI 115 A-SM             Organismal Biology                 F   \n",
       "7370      14/D     PY 101 F     Introduction to Psychology                 F   \n",
       "7371      14/D     EH 102 C  Sem Critical Thinking Writing                 F   \n",
       "1114      14/D     MS 120 D                          Voice                 F   \n",
       "\n",
       "      Sttr Stu Credits  Sttr Attempted Cred Sttr Stu Final Grades  \\\n",
       "6784              1.00                  NaN                     F   \n",
       "7369              1.00                  3.0                     F   \n",
       "7370              1.00                  NaN                    C-   \n",
       "7371              1.00                  NaN                     F   \n",
       "1114              0.25                  NaN                     A   \n",
       "\n",
       "      Sttr Cmpl Cred  Sttr Term Gpa            faculty  ...  FA5 Enr  \\\n",
       "6784            1.00          1.110  Sandra Sprayberry  ...      NaN   \n",
       "7369            1.00          0.557      Andrew Gannon  ...      NaN   \n",
       "7370            1.00          0.557    Gabrielle Smith  ...      NaN   \n",
       "7371            1.00          0.557      Steven Carter  ...      NaN   \n",
       "1114            3.25          3.591       Erin Ludwick  ...      NaN   \n",
       "\n",
       "     FA5 Class FA5 Major  FA6 Enr FA6 Class FA6 Major  Graduated Grad Term  \\\n",
       "6784       NaN       NaN      NaN       NaN       NaN          1     18/SP   \n",
       "7369       NaN       NaN      NaN       NaN       NaN          0       NaN   \n",
       "7370       NaN       NaN      NaN       NaN       NaN          0       NaN   \n",
       "7371       NaN       NaN      NaN       NaN       NaN          0       NaN   \n",
       "1114       NaN       NaN      NaN       NaN       NaN          1     18/SP   \n",
       "\n",
       "     Grad Year                   Grad Major  \n",
       "6784    1718.0  Urban Environmental Studies  \n",
       "7369       NaN                          NaN  \n",
       "7370       NaN                          NaN  \n",
       "7371       NaN                          NaN  \n",
       "1114    1718.0                      Biology  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def preprocessing(data):\n",
    "    #changing nomenclature for term so that we can sort in chronological order by that column\n",
    "df['Sttr Term'] = df['Sttr Term'].replace({'WI':'A','SP':'B', 'SU':'C', 'FA':'D'}, regex=True)\n",
    "    #dropping completely null rows\n",
    "df.drop(df.index[51518:51537], inplace=True)\n",
    "    #isolating students who would have been able to graduate in the timeframe of our given data\n",
    "df = df.loc[(df['Cohort'] == '14/FA') | (df['Cohort'] == '15/FA') | (df['Cohort'] == '16/FA')\n",
    "              | (df['Cohort'] == '17/FA')]\n",
    "df = df.dropna(subset=['Sttr Term'])\n",
    "df['alt_id'] = df['alt_id'].astype(int)\n",
    "    #encoding our target\n",
    "mapping = {'N': 0, 'Y': 1}\n",
    "df['Graduated'] = df['Graduated'].map(mapping)\n",
    "df = df.sort_values(['Sttr Term'], ascending=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139    64\n",
       "1469    62\n",
       "2046    62\n",
       "1002    62\n",
       "2328    60\n",
       "        ..\n",
       "2030     3\n",
       "2516     3\n",
       "1732     3\n",
       "1191     3\n",
       "1857     3\n",
       "Name: alt_id, Length: 1455, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#investigating how many classes students are taking to find outliers\n",
    "df.alt_id.value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploration Project               3097\n",
      "Sem Critical Thinking Writing     1210\n",
      "Introduction to Psychology         973\n",
      "Calculus I                         660\n",
      "Introduction to Sociology          651\n",
      "                                  ... \n",
      "Panorama of Spanish Literature       1\n",
      "ED*499 Internship I                  1\n",
      "Advanced Conversational Arabic       1\n",
      "SPTP: Italian Cinema                 1\n",
      "Black Studies Capstone               1\n",
      "Name: course_title, Length: 952, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for Course Title Duplicates\n",
    "print(df['course_title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['course_title'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding Course Title ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #data = df_4[['course_title']]\n",
    "#encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    #encoded = OneHotEncoder(sparse=False)\n",
    "#encoder.fit(df)\n",
    "    #encoded_df = encoded.fit_transform(data)\n",
    "#df_encoded = encoder.transform(df)\n",
    "    #encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohe = OneHotEncoder()\n",
    "#dummies = ohe.fit_transform(df_4[['course_title']])\n",
    "#dummies_df = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(), index=df_4.index)\n",
    "#ohe_df = pd.concat([df_4, dummies_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Target and Feature Variables ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#y = df['Graduated']\n",
    "#X = df.drop(columns=['Graduated'], axis=1)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Semester Term - Ascending ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('course_title==0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['Sttr Term'], ascending=True)\n",
    "\n",
    "student_ids = df.alt_id.unique()\n",
    "\n",
    "course_sequences = []\n",
    "\n",
    "for s_id in student_ids:\n",
    "    frame = df[df['alt_id'] == s_id]\n",
    "    course_ids = ' '.join(frame.course_title.str.replace(' ', '').to_list())\n",
    "    course_sequences.append(course_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(course_sequences)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#tokenizer = Tokenizer()\n",
    "\n",
    "#tokenizer.fit_on_texts(course_sequences)\n",
    "\n",
    "#encoded_sequences = tokenizer.texts_to_sequences(course_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#padded_sequences = pad_sequences(\n",
    "  # encoded_sequences, maxlen=None, dtype='int32', padding='post',\n",
    "    #truncating='post', value=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#feature_ids = list(tokenizer.word_index.values())\n",
    "\n",
    "#feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "\n",
    "#feature_encoder.transform(np.array(padded_sequences[0]).reshape(-1,1)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padded_reshaped = padded_sequences.reshape(1455, 80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#model = Sequential()\n",
    "#input_layer = Input(shape=(80, 1))\n",
    "#sequential_1 = LSTM(64, activation='relu')\n",
    "\n",
    "#output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "#model.add(input_layer)\n",
    "#model.add(sequential_1)\n",
    "#model.add(output_layer)\n",
    "\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(padded_reshaped, target, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking out course_title next to course names in column titles ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def preprocessing_courses(data):\n",
    "    #data.drop(data.index[51518:51537], inplace=True)\n",
    "    #data = data.loc[(data['Cohort'] == '14/FA') | (data['Cohort'] == '15/FA') | (data['Cohort'] == '16/FA')\n",
    "     #         | (data['Cohort'] == '17/FA')]\n",
    "   # data = data.dropna(subset=['Sttr Term'])\n",
    "   # mapping = {'N': 0, 'Y': 1}\n",
    "   # data['Graduated'] = data['Graduated'].map(mapping)\n",
    "   # y = data['Graduated']\n",
    "   # X = data['course_title']\n",
    "   # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2021)\n",
    "   # X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)\n",
    "   # data['alt_id'] = data['alt_id'].astype(int)\n",
    "   # df = data.sort_values(['Sttr Term'], ascending=True)\n",
    "    #return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for s_id in student_ids:\n",
    "    grad = df[df['alt_id'] == s_id].iloc[0].Graduated\n",
    "    target.append(grad)\n",
    "    \n",
    "def modeling (data):\n",
    "    student_ids = data.alt_id.unique()\n",
    "    course_sequences = []\n",
    "    for s_id in student_ids:\n",
    "        frame = data[data['alt_id'] == s_id]\n",
    "        course_ids = ' '.join(frame.course_title.str.replace(' ', '').to_list())\n",
    "        course_sequences.append(course_ids)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(course_sequences)\n",
    "    encoded_sequences = tokenizer.texts_to_sequences(course_sequences)\n",
    "    padded_sequences = pad_sequences(\n",
    "    encoded_sequences, maxlen=None, dtype='int32', padding='post',\n",
    "    truncating='post', value=0.0)\n",
    "    feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    feature_ids = list(tokenizer.word_index.values())\n",
    "    feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "    feature_encoder.transform(np.array(padded_sequences[0]).reshape(-1,1)).reshape(-1,1)\n",
    "    padded_reshaped = padded_sequences.reshape(1455, 45, 1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    input_layer = Input(shape=(80, 1))\n",
    "    sequential_1 = LSTM(64, activation='relu')\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "    model.add(input_layer)\n",
    "    model.add(sequential_1)\n",
    "    model.add(output_layer)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit(padded_reshaped, target, epochs=10)\n",
    "    pd.DataFrame(feature_encoder.transform(np.array(padded_sequences[0]).reshape(-1,1)).todense())\n",
    "    return np.array(padded_sequences[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Course Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for s_id in student_ids:\n",
    "    grad = df[df['alt_id'] == s_id].iloc[0].Graduated\n",
    "    target.append(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 80, 1) for input Tensor(\"input_14:0\", shape=(None, 80, 1), dtype=float32), but it was called on an input with incompatible shape (None, 45, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 80, 1) for input Tensor(\"input_14:0\", shape=(None, 80, 1), dtype=float32), but it was called on an input with incompatible shape (None, 45, 1).\n",
      "37/37 [==============================] - ETA: 0s - loss: 27.1097 - accuracy: 0.3411WARNING:tensorflow:Model was constructed with shape (None, 80, 1) for input Tensor(\"input_14:0\", shape=(None, 80, 1), dtype=float32), but it was called on an input with incompatible shape (None, 45, 1).\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 27.1097 - accuracy: 0.3411 - val_loss: 17.4348 - val_accuracy: 0.4089\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 15.8508 - accuracy: 0.4785 - val_loss: 10.5783 - val_accuracy: 0.4570\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 11.8984 - accuracy: 0.5052 - val_loss: 7.2697 - val_accuracy: 0.5120\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 6.7395 - accuracy: 0.5713 - val_loss: 7.8343 - val_accuracy: 0.5086\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 7.3227 - accuracy: 0.6065 - val_loss: 9.3956 - val_accuracy: 0.5120\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 13.5577 - accuracy: 0.5997 - val_loss: 12.4444 - val_accuracy: 0.6632\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 14.0710 - accuracy: 0.8239 - val_loss: 9.0772 - val_accuracy: 0.7835\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 12.2578 - accuracy: 0.7423 - val_loss: 23.9273 - val_accuracy: 0.7457\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 14.1311 - accuracy: 0.7251 - val_loss: 10.5610 - val_accuracy: 0.7766\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 5.6664 - accuracy: 0.8316 - val_loss: 6.7329 - val_accuracy: 0.7766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 40],\n",
       "       [ 13],\n",
       "       [ 12],\n",
       "       [ 10],\n",
       "       [  1],\n",
       "       [ 65],\n",
       "       [191],\n",
       "       [102],\n",
       "       [  4],\n",
       "       [ 10],\n",
       "       [334],\n",
       "       [134],\n",
       "       [ 43],\n",
       "       [ 40],\n",
       "       [542],\n",
       "       [424],\n",
       "       [  6],\n",
       "       [ 50],\n",
       "       [341],\n",
       "       [134],\n",
       "       [323],\n",
       "       [  3],\n",
       "       [ 85],\n",
       "       [  3],\n",
       "       [  1],\n",
       "       [413],\n",
       "       [425],\n",
       "       [  9],\n",
       "       [543],\n",
       "       [789],\n",
       "       [ 22],\n",
       "       [ 16],\n",
       "       [ 47],\n",
       "       [371],\n",
       "       [485],\n",
       "       [400],\n",
       "       [401],\n",
       "       [304],\n",
       "       [624],\n",
       "       [402],\n",
       "       [527],\n",
       "       [181],\n",
       "       [790],\n",
       "       [  0],\n",
       "       [  0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline Course Sequence\n",
    "student_ids = df.alt_id.unique()\n",
    "course_sequences = []\n",
    "for s_id in student_ids:\n",
    "    frame = df[df['alt_id'] == s_id]\n",
    "    course_ids = ' '.join(frame.course_title.str.replace(' ', '').to_list())\n",
    "    course_sequences.append(course_ids)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(course_sequences)\n",
    "encoded_sequences = tokenizer.texts_to_sequences(course_sequences)\n",
    "padded_course_sequences = pad_sequences(encoded_sequences, maxlen=45, dtype='int32', padding='post',\n",
    "truncating='post', value=0.0)\n",
    "feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "feature_ids = list(tokenizer.word_index.values())\n",
    "feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "feature_encoder.transform(np.array(padded_course_sequences[0]).reshape(-1,1)).reshape(-1,1)\n",
    "padded_reshaped_courses = padded_course_sequences.reshape(1455, 45, 1)\n",
    "\n",
    "model = Sequential()\n",
    "input_layer = Input(shape=(80, 1))\n",
    "sequential_1 = LSTM(64, activation='relu')\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "model.add(input_layer)\n",
    "model.add(sequential_1)\n",
    "model.add(output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(padded_reshaped_courses, np.array(target), epochs=10, validation_split = .2)\n",
    "\n",
    "\n",
    "pd.DataFrame(feature_encoder.transform(np.array(padded_course_sequences[0]).reshape(-1,1)).todense())\n",
    "np.array(padded_course_sequences[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for s_id in student_ids:\n",
    "    grad = df[df['alt_id'] == s_id].iloc[0].Graduated\n",
    "    target.append(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.5903 - accuracy: 0.6641 - val_loss: 0.6327 - val_accuracy: 0.5601\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.4227 - accuracy: 0.7612 - val_loss: 0.5662 - val_accuracy: 0.8522\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.3575 - accuracy: 0.9210 - val_loss: 0.3403 - val_accuracy: 0.9072\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.3724 - accuracy: 0.9012 - val_loss: 0.4378 - val_accuracy: 0.8591\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.3681 - accuracy: 0.9364 - val_loss: 0.4368 - val_accuracy: 0.8729\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.2708 - accuracy: 0.9330 - val_loss: 0.3238 - val_accuracy: 0.8797\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 0.1763 - accuracy: 0.9476 - val_loss: 0.2415 - val_accuracy: 0.9107\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 0.1818 - accuracy: 0.9493 - val_loss: 0.2334 - val_accuracy: 0.9244\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.1644 - accuracy: 0.9536 - val_loss: 0.2241 - val_accuracy: 0.9141\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.1506 - accuracy: 0.9553 - val_loss: 0.2392 - val_accuracy: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb808e22340>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline grades\n",
    "student_ids = df.alt_id.unique()\n",
    "grade_sequences = []\n",
    "for s_id in student_ids:\n",
    "    frame = df[df['alt_id'] == s_id]\n",
    "    grades = ' '.join(frame['Sttr Stu Final Grades'].to_list())\n",
    "    grade_sequences.append(grades)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(grade_sequences)\n",
    "encoded_sequences = tokenizer.texts_to_sequences(grade_sequences)\n",
    "padded_grade_sequences = pad_sequences(\n",
    "encoded_sequences, maxlen=45, dtype='int32', padding='post',\n",
    "truncating='post', value=0.0)\n",
    "feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "feature_ids = list(tokenizer.word_index.values())\n",
    "feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "feature_encoder.transform(np.array(padded_grade_sequences[0]).reshape(-1,1)).reshape(-1,1)\n",
    "padded_reshaped_grades = padded_grade_sequences.reshape(1455, 45, 1)\n",
    "\n",
    "model = Sequential()\n",
    "input_layer = Input(shape=(45, 1))\n",
    "sequential_1 = LSTM(64, activation='relu')\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "model.add(input_layer)\n",
    "model.add(sequential_1)\n",
    "model.add(output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(padded_reshaped_grades, np.array(target), epochs=10, validation_split = .2)\n",
    "\n",
    "\n",
    "#pd.DataFrame(feature_encoder.transform(np.array(padded_grade_sequences[0]).reshape(-1,1)).todense())\n",
    "#np.array(padded_grade_sequences[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 72,  10,   2,  65,   1,   5,   4, 288,   8,  14,  82,   1,  60,\n",
       "         3,  29,  23, 101,  52, 206,   6, 110,   1,  71, 272, 360,  97,\n",
       "       672, 673, 590,  67, 243, 442, 135, 136, 137, 197, 516,  30, 244,\n",
       "       197, 286, 499, 500,  86,   0], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_course_sequences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 2, 2, 2, 4, 2, 3, 4, 3, 2, 1, 2, 6, 3, 1, 2, 1, 1, 5, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 2, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_grade_sequences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooray = np.stack((padded_grade_sequences, padded_course_sequences),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 45, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Two sequential, label encoded features\n",
    "X = hooray\n",
    "Y = target\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,Y,random_state=2021)\n",
    "\n",
    "###Define model###\n",
    "inp = layers.Input((45,2))\n",
    "feature1 = layers.Lambda(lambda x: x[:,:,0])(inp)\n",
    "feature2 = layers.Lambda(lambda x: x[:,:,1])(inp)\n",
    "\n",
    "#Append embeddings features\n",
    "x1 = layers.Embedding(1445, 5)(feature1)\n",
    "x2 = layers.Embedding(1445, 7)(feature2)\n",
    "\n",
    "#LSTMs\n",
    "feature1 = layers.LSTM(8, return_sequences=True)(x1)\n",
    "feature1 = layers.LSTM(8)(feature1)\n",
    "\n",
    "feature2 = layers.LSTM(8, return_sequences=True)(x2)\n",
    "feature2 = layers.LSTM(8)(feature2)\n",
    "\n",
    "#Combine LSTM final states\n",
    "x = layers.concatenate([x1,x2])\n",
    "out = layers.Dense(1)(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "utils.plot_model(model, show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.4512 - accuracy: 0.3359 - val_loss: 1.2990 - val_accuracy: 0.4399\n",
      "Epoch 2/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.3834 - accuracy: 0.3359 - val_loss: 1.0970 - val_accuracy: 0.4399\n",
      "Epoch 3/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2001 - accuracy: 0.3359 - val_loss: 0.9833 - val_accuracy: 0.4399\n",
      "Epoch 4/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.0696 - accuracy: 0.3359 - val_loss: 0.8950 - val_accuracy: 0.4399\n",
      "Epoch 5/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9621 - accuracy: 0.3391 - val_loss: 0.8203 - val_accuracy: 0.4580\n",
      "Epoch 6/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.8695 - accuracy: 0.3666 - val_loss: 0.7581 - val_accuracy: 0.4874\n",
      "Epoch 7/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7905 - accuracy: 0.4266 - val_loss: 0.7046 - val_accuracy: 0.5371\n",
      "Epoch 8/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.7213 - accuracy: 0.5242 - val_loss: 0.6604 - val_accuracy: 0.6021\n",
      "Epoch 9/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.5984 - val_loss: 0.6253 - val_accuracy: 0.6509\n",
      "Epoch 10/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.6141 - accuracy: 0.6661 - val_loss: 0.6004 - val_accuracy: 0.6842\n",
      "Epoch 11/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5751 - accuracy: 0.7078 - val_loss: 0.5930 - val_accuracy: 0.7014\n",
      "Epoch 12/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5467 - accuracy: 0.7392 - val_loss: 0.5858 - val_accuracy: 0.7208\n",
      "Epoch 13/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7603 - val_loss: 0.5848 - val_accuracy: 0.7317\n",
      "Epoch 14/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7758 - val_loss: 0.5872 - val_accuracy: 0.7378\n",
      "Epoch 15/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7844 - val_loss: 0.5893 - val_accuracy: 0.7430\n",
      "Epoch 16/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4931 - accuracy: 0.7926 - val_loss: 0.5989 - val_accuracy: 0.7444\n",
      "Epoch 17/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.7981 - val_loss: 0.6026 - val_accuracy: 0.7479\n",
      "Epoch 18/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.8005 - val_loss: 0.6105 - val_accuracy: 0.7500\n",
      "Epoch 19/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4744 - accuracy: 0.8036 - val_loss: 0.6098 - val_accuracy: 0.7517\n",
      "Epoch 20/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.8086 - val_loss: 0.6071 - val_accuracy: 0.7536\n",
      "Epoch 21/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.8095 - val_loss: 0.6115 - val_accuracy: 0.7549\n",
      "Epoch 22/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.8117 - val_loss: 0.6132 - val_accuracy: 0.7559\n",
      "Epoch 23/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.8141 - val_loss: 0.6158 - val_accuracy: 0.7578\n",
      "Epoch 24/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8173 - val_loss: 0.6162 - val_accuracy: 0.7586\n",
      "Epoch 25/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.8188 - val_loss: 0.6158 - val_accuracy: 0.7595\n",
      "Epoch 26/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.8190 - val_loss: 0.6161 - val_accuracy: 0.7601\n",
      "Epoch 27/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4504 - accuracy: 0.8194 - val_loss: 0.6181 - val_accuracy: 0.7611\n",
      "Epoch 28/30\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.8206 - val_loss: 0.6197 - val_accuracy: 0.7618\n",
      "Epoch 29/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.8210 - val_loss: 0.6208 - val_accuracy: 0.7623\n",
      "Epoch 30/30\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4463 - accuracy: 0.8223 - val_loss: 0.6224 - val_accuracy: 0.7627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb811502e50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X,Y, epochs=30, validation_split = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.4512054920196533,\n",
       "  1.3833792209625244,\n",
       "  1.2001181840896606,\n",
       "  1.069641351699829,\n",
       "  0.9620771408081055,\n",
       "  0.8695030212402344,\n",
       "  0.7905457615852356,\n",
       "  0.7213184833526611,\n",
       "  0.6628780961036682,\n",
       "  0.6141045093536377,\n",
       "  0.5751020908355713,\n",
       "  0.5466899275779724,\n",
       "  0.5278627872467041,\n",
       "  0.513586163520813,\n",
       "  0.5014046430587769,\n",
       "  0.49311545491218567,\n",
       "  0.4857247471809387,\n",
       "  0.4791771471500397,\n",
       "  0.474350243806839,\n",
       "  0.4699798822402954,\n",
       "  0.46653714776039124,\n",
       "  0.462510347366333,\n",
       "  0.4597660303115845,\n",
       "  0.4570811688899994,\n",
       "  0.45533332228660583,\n",
       "  0.45255669951438904,\n",
       "  0.4503863751888275,\n",
       "  0.4492607116699219,\n",
       "  0.44767898321151733,\n",
       "  0.4462898373603821],\n",
       " 'accuracy': [0.3359106481075287,\n",
       "  0.3359106481075287,\n",
       "  0.3359106481075287,\n",
       "  0.3359106481075287,\n",
       "  0.3390989601612091,\n",
       "  0.3666093945503235,\n",
       "  0.42657506465911865,\n",
       "  0.5241504907608032,\n",
       "  0.5983772277832031,\n",
       "  0.6660557985305786,\n",
       "  0.7077891826629639,\n",
       "  0.7392132878303528,\n",
       "  0.7603474855422974,\n",
       "  0.7758303880691528,\n",
       "  0.7844216227531433,\n",
       "  0.7926498651504517,\n",
       "  0.7980719208717346,\n",
       "  0.8004582524299622,\n",
       "  0.8035510778427124,\n",
       "  0.8085529208183289,\n",
       "  0.8095265626907349,\n",
       "  0.8117029666900635,\n",
       "  0.8140893578529358,\n",
       "  0.8173158168792725,\n",
       "  0.818766713142395,\n",
       "  0.8190340399742126,\n",
       "  0.8193967938423157,\n",
       "  0.8206376433372498,\n",
       "  0.8210003972053528,\n",
       "  0.8223176002502441],\n",
       " 'val_loss': [1.2989832162857056,\n",
       "  1.0970491170883179,\n",
       "  0.9833114743232727,\n",
       "  0.8950148224830627,\n",
       "  0.820252001285553,\n",
       "  0.7581105828285217,\n",
       "  0.7046266198158264,\n",
       "  0.6604392528533936,\n",
       "  0.6252869963645935,\n",
       "  0.6004007458686829,\n",
       "  0.5929848551750183,\n",
       "  0.5857598185539246,\n",
       "  0.5847784876823425,\n",
       "  0.5871712565422058,\n",
       "  0.5893287062644958,\n",
       "  0.5988882184028625,\n",
       "  0.6025545597076416,\n",
       "  0.6104613542556763,\n",
       "  0.6097850203514099,\n",
       "  0.6070587038993835,\n",
       "  0.6115374565124512,\n",
       "  0.6132164001464844,\n",
       "  0.6157597303390503,\n",
       "  0.6161510348320007,\n",
       "  0.6157506704330444,\n",
       "  0.6161115765571594,\n",
       "  0.618110716342926,\n",
       "  0.6197302937507629,\n",
       "  0.620786726474762,\n",
       "  0.622431755065918],\n",
       " 'val_accuracy': [0.43986254930496216,\n",
       "  0.43986254930496216,\n",
       "  0.43986254930496216,\n",
       "  0.43986254930496216,\n",
       "  0.4579610824584961,\n",
       "  0.4873615801334381,\n",
       "  0.5370751619338989,\n",
       "  0.6021382212638855,\n",
       "  0.6508590579032898,\n",
       "  0.6842305660247803,\n",
       "  0.7014127373695374,\n",
       "  0.7208094596862793,\n",
       "  0.7317296862602234,\n",
       "  0.737838864326477,\n",
       "  0.7429553866386414,\n",
       "  0.7444062232971191,\n",
       "  0.7479190230369568,\n",
       "  0.7499809861183167,\n",
       "  0.7516608834266663,\n",
       "  0.753570020198822,\n",
       "  0.7548682689666748,\n",
       "  0.755937397480011,\n",
       "  0.7577701210975647,\n",
       "  0.7586100697517395,\n",
       "  0.7594501376152039,\n",
       "  0.7600610852241516,\n",
       "  0.761053740978241,\n",
       "  0.761817455291748,\n",
       "  0.7622756958007812,\n",
       "  0.7626574635505676]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for s_id in student_ids:\n",
    "    grad = df[df['alt_id'] == s_id].iloc[0].Graduated\n",
    "    target.append(grad)\n",
    "    \n",
    "student_ids = df.alt_id.unique()\n",
    "course_sequences = []\n",
    "for s_id in student_ids:\n",
    "    frame = df[df['alt_id'] == s_id]\n",
    "    term_sequences = []\n",
    "    for idx, val in enumerate(frame['Sttr Term'].unique()):\n",
    "        term = frame[frame['Sttr Term'] == val].sort_values('course_title')\n",
    "        course_group = term.course_title.str.replace(' ', '').str.cat(sep='')\n",
    "        term_sequences.append(course_group)\n",
    "    term_sequences = ' '.join(term_sequences)\n",
    "    course_sequences.append(term_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IntrotoMediaStudiesMusicinFilmSemCriticalThinkingWriting ClassVoiceFilmProductionIIntroductiontoFictionIntroductiontoPsychology'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IntermediateWritingIntrotoAmerGovt&PolOrganismalBiology ExplorationProject HistoryofAmericanPeopleIIIntroductiontoPsychologyOrganismalBiologyPhilosophyofReligionSkillsinChemistry IntermediateWritingIntroGlobalCompStudiesLogicWhyPeopBelieveWeirdThings ContemporaryPhilosophyIntroductiontoSociologyWhyWeCareAboutthePuritans IntroEnvironmentalStudies EuropeanCivilII(IA)HistofWesternPhilosophyILogicMidEastin20thCent(IA) ExplorationProject EnvironmentalProbandPolicyPrinofMicroeconomicsReligionandtheNaturalWorldUrbanSociology Two-DimensionalDesignUESInternship AsianPersponEnvironScienceEvolutionaryEcologyModernPhilosophyR3urbanPlan,Develop&Design EnvironmentalEarthSciencesEnvironmentalFieldMethodsSrSeminarinEnvironStudiesTchgExpinUrbanEnvStudiesTopicsinAmericanPhilosophy'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 80, 1) for input Tensor(\"input_27:0\", shape=(None, 80, 1), dtype=float32), but it was called on an input with incompatible shape (None, 45, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 80, 1) for input Tensor(\"input_27:0\", shape=(None, 80, 1), dtype=float32), but it was called on an input with incompatible shape (None, 45, 1).\n",
      "36/37 [============================>.] - ETA: 0s - loss: 93.0622 - accuracy: 0.5747WARNING:tensorflow:Model was constructed with shape (None, 80, 1) for input Tensor(\"input_27:0\", shape=(None, 80, 1), dtype=float32), but it was called on an input with incompatible shape (None, 45, 1).\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 102.1583 - accuracy: 0.5704 - val_loss: 289.9015 - val_accuracy: 0.5052\n",
      "Epoch 2/20\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 190.8974 - accuracy: 0.5077 - val_loss: 151.0368 - val_accuracy: 0.5636\n",
      "Epoch 3/20\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 156.4861 - accuracy: 0.5275 - val_loss: 440.9733 - val_accuracy: 0.5052\n",
      "Epoch 4/20\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 286.0645 - accuracy: 0.4871 - val_loss: 314.6613 - val_accuracy: 0.5086\n",
      "Epoch 5/20\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 164.1468 - accuracy: 0.5619 - val_loss: 272.7901 - val_accuracy: 0.5533\n",
      "Epoch 6/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 206.4371 - accuracy: 0.6186 - val_loss: 536.5527 - val_accuracy: 0.6220\n",
      "Epoch 7/20\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 255.5276 - accuracy: 0.6572 - val_loss: 714.4623 - val_accuracy: 0.5808\n",
      "Epoch 8/20\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 181.9834 - accuracy: 0.6392 - val_loss: 202.5568 - val_accuracy: 0.5464\n",
      "Epoch 9/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 184.4777 - accuracy: 0.6770 - val_loss: 410.3831 - val_accuracy: 0.5979\n",
      "Epoch 10/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 185.9630 - accuracy: 0.6314 - val_loss: 588.3868 - val_accuracy: 0.5739\n",
      "Epoch 11/20\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 450.3631 - accuracy: 0.5644 - val_loss: 442.2809 - val_accuracy: 0.4536\n",
      "Epoch 12/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 506.0828 - accuracy: 0.4966 - val_loss: 597.1829 - val_accuracy: 0.4914\n",
      "Epoch 13/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 285.9310 - accuracy: 0.4691 - val_loss: 347.7433 - val_accuracy: 0.4605\n",
      "Epoch 14/20\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 335.2675 - accuracy: 0.5352 - val_loss: 1320.6848 - val_accuracy: 0.3436\n",
      "Epoch 15/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 930.5744 - accuracy: 0.5000 - val_loss: 1243.5403 - val_accuracy: 0.4674\n",
      "Epoch 16/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 401.4710 - accuracy: 0.5601 - val_loss: 201.5701 - val_accuracy: 0.5533\n",
      "Epoch 17/20\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 99.4465 - accuracy: 0.5515 - val_loss: 315.1267 - val_accuracy: 0.5052\n",
      "Epoch 18/20\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 285.4941 - accuracy: 0.5773 - val_loss: 477.9451 - val_accuracy: 0.5739\n",
      "Epoch 19/20\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 367.1465 - accuracy: 0.6014 - val_loss: 2217.8694 - val_accuracy: 0.5704\n",
      "Epoch 20/20\n",
      "37/37 [==============================] - 0s 13ms/step - loss: 409.9642 - accuracy: 0.6263 - val_loss: 487.5325 - val_accuracy: 0.5704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[360],\n",
       "       [979],\n",
       "       [  1],\n",
       "       [980],\n",
       "       [981],\n",
       "       [982],\n",
       "       [ 39],\n",
       "       [ 17],\n",
       "       [  2],\n",
       "       [983],\n",
       "       [  2],\n",
       "       [  1],\n",
       "       [984],\n",
       "       [ 97],\n",
       "       [985],\n",
       "       [986],\n",
       "       [ 66],\n",
       "       [120],\n",
       "       [987],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0]], dtype=int32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(course_sequences)\n",
    "encoded_sequences = tokenizer.texts_to_sequences(course_sequences)\n",
    "padded_course_sequences = pad_sequences(\n",
    "encoded_sequences, maxlen=45, dtype='int32', padding='post',\n",
    "truncating='post', value=0.0)\n",
    "feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "feature_ids = list(tokenizer.word_index.values())\n",
    "feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "feature_encoder.transform(np.array(padded_course_sequences[0]).reshape(-1,1)).reshape(-1,1)\n",
    "padded_reshaped_sequences = padded_course_sequences.reshape(1455, 45, 1)\n",
    "\n",
    "model = Sequential()\n",
    "input_layer = Input(shape=(80, 1))\n",
    "sequential_1 = LSTM(64, activation='relu')\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "model.add(input_layer)\n",
    "model.add(sequential_1)\n",
    "model.add(output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(padded_reshaped_sequences, np.array(target), epochs=20, validation_split = .2)\n",
    "\n",
    "\n",
    "pd.DataFrame(feature_encoder.transform(np.array(padded_reshaped_sequences[0]).reshape(-1,1)).todense())\n",
    "np.array(padded_course_sequences[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_courses(data):\n",
    "    data.drop(data.index[51518:51537], inplace=True)\n",
    "    data = data.loc[(data['Cohort'] == '14/FA') | (data['Cohort'] == '15/FA') | (data['Cohort'] == '16/FA')\n",
    "              | (data['Cohort'] == '17/FA')]\n",
    "    data = data.dropna(subset=['Sttr Term'])\n",
    "    mapping = {'N': 0, 'Y': 1}\n",
    "    data['Graduated'] = data['Graduated'].map(mapping)\n",
    "    y = data['Graduated']\n",
    "    X = data['course_title']\n",
    "    data['alt_id'] = data['alt_id'].astype(int)\n",
    "    df = data.sort_values(['Sttr Term'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(df.index[51518:51537], inplace=True)\n",
    "df = df.loc[(df['Cohort'] == '14/FA') | (df['Cohort'] == '15/FA') | (df['Cohort'] == '16/FA')\n",
    "              | (df['Cohort'] == '17/FA')]\n",
    "df = df.dropna(subset=['Sttr Term'])\n",
    "#mapping = {'N': 0, 'Y': 1}\n",
    "#df['Graduated'] = df['Graduated'].map(mapping)\n",
    "#y = df['Graduated']\n",
    "#X = df['course_title']\n",
    "df['alt_id'] = df['alt_id'].astype(int)\n",
    "df = df.sort_values(['Sttr Term'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regulating Course Sequence\n",
    "target = []\n",
    "for s_id in student_ids:\n",
    "    grad = df[df['alt_id'] == s_id].iloc[0].Graduated\n",
    "    target.append(grad)\n",
    "    \n",
    "student_ids = df.alt_id.unique()\n",
    "course_sequences = []\n",
    "for s_id in student_ids:\n",
    "    frame = df[df['alt_id'] == s_id]\n",
    "    course_ids = ' '.join(frame.course_title.str.replace(' ', '').to_list())\n",
    "    course_sequences.append(course_ids)\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(course_sequences)\n",
    "\n",
    "encoded_sequences = tokenizer.texts_to_sequences(course_sequences)\n",
    "padded_course_sequences = pad_sequences(encoded_sequences, maxlen=45, dtype='int32', padding='post',\n",
    "truncating='post', value=0.0)\n",
    "feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "feature_ids = list(tokenizer.word_index.values())\n",
    "feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "feature_encoder.transform(np.array(padded_course_sequences[0]).reshape(-1,1)).reshape(-1,1)\n",
    "padded_reshaped_courses = padded_course_sequences.reshape(1455, 45, 1)\n",
    "\n",
    "X = padded_reshaped_courses\n",
    "y = target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, random_state=2021)\n",
    "\n",
    "\n",
    "X_train_tokens = (X_train_final)\n",
    "X_val_tokens = (X_val)\n",
    "X_test_tokens = (X_test)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final)\n",
    "y_train_lb = to_categorical(lb.transform(y_train_final))[:, 1]\n",
    "y_val_lb = to_categorical(lb.transform(y_val))[:, 1]\n",
    "y_test_lb = to_categorical(lb.transform(y_test))[:, 1]\n",
    "\n",
    "baseline_model = Sequential()\n",
    "input_layer = Input(shape=(80, 1))\n",
    "sequential_1 = LSTM(64, activation='relu')\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "baseline_model.add(input_layer)\n",
    "baseline_model.add(sequential_1)\n",
    "baseline_model.add(output_layer)\n",
    "\n",
    "baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "baseline_model_val = baseline_model.fit(X_train_tokens, y_train_lb, epochs=150, batch_size=250, validation_data = (X_val_tokens, y_val_lb))\n",
    "\n",
    "#pd.DataFrame(feature_encoder.transform(np.array(padded_course_sequences[0]).reshape(-1,1)).todense())\n",
    "#np.array(padded_course_sequences[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for s_id in student_ids:\n",
    "    grad = df[df['alt_id'] == s_id].iloc[0].Graduated\n",
    "    target.append(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1455, 1) for input Tensor(\"input_45:0\", shape=(None, 1455, 1), dtype=float32), but it was called on an input with incompatible shape (None, 16, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1455, 1) for input Tensor(\"input_45:0\", shape=(None, 1455, 1), dtype=float32), but it was called on an input with incompatible shape (None, 16, 1).\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.5840 - accuracy: 0.8075WARNING:tensorflow:Model was constructed with shape (None, 1455, 1) for input Tensor(\"input_45:0\", shape=(None, 1455, 1), dtype=float32), but it was called on an input with incompatible shape (None, 16, 1).\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5692 - accuracy: 0.8162 - val_loss: 0.6092 - val_accuracy: 0.7766\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.4949 - accuracy: 0.8565 - val_loss: 0.5242 - val_accuracy: 0.7869\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.4266 - accuracy: 0.8548 - val_loss: 0.4695 - val_accuracy: 0.7904\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3866 - accuracy: 0.8565 - val_loss: 0.4179 - val_accuracy: 0.8041\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3599 - accuracy: 0.8634 - val_loss: 0.4278 - val_accuracy: 0.8179\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.8746 - val_loss: 0.4624 - val_accuracy: 0.8247\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3357 - accuracy: 0.8746 - val_loss: 0.3898 - val_accuracy: 0.8351\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3282 - accuracy: 0.8763 - val_loss: 0.4026 - val_accuracy: 0.8179\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.8823 - val_loss: 0.3646 - val_accuracy: 0.8385\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3112 - accuracy: 0.8857 - val_loss: 0.3694 - val_accuracy: 0.8488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb802453fd0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sophomore Baseline Grades\n",
    "student_ids = df.alt_id.unique()\n",
    "grade_sequences = []\n",
    "for s_id in student_ids:\n",
    "    frame = df[df['alt_id'] == s_id]\n",
    "    grades = ' '.join(frame['Sttr Stu Final Grades'].to_list())\n",
    "    grade_sequences.append(grades)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(grade_sequences)\n",
    "encoded_sequences = tokenizer.texts_to_sequences(grade_sequences)\n",
    "padded_grade_sequences = pad_sequences(\n",
    "encoded_sequences, maxlen=16, dtype='int32', padding='post',\n",
    "truncating='post', value=0.0)\n",
    "feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "feature_ids = list(tokenizer.word_index.values())\n",
    "feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "feature_encoder.transform(np.array(padded_grade_sequences[0]).reshape(-1,1)).reshape(-1,1)\n",
    "padded_reshaped_grades = padded_grade_sequences.reshape(1455, 16, 1)\n",
    "\n",
    "model = Sequential()\n",
    "input_layer = Input(shape=(1455, 1))\n",
    "sequential_1 = LSTM(64, activation='relu')\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "model.add(input_layer)\n",
    "model.add(sequential_1)\n",
    "model.add(output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(padded_reshaped_grades, np.array(target), epochs=10, validation_split = .2)\n",
    "\n",
    "\n",
    "#pd.DataFrame(feature_encoder.transform(np.array(padded_grade_sequences[0]).reshape(-1,1)).todense())\n",
    "#np.array(padded_grade_sequences[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for s_id in student_ids:\n",
    "    grad = df[df['alt_id'] == s_id].iloc[0].Graduated\n",
    "    target.append(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1455, 1) for input Tensor(\"input_49:0\", shape=(None, 1455, 1), dtype=float32), but it was called on an input with incompatible shape (None, 4, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1455, 1) for input Tensor(\"input_49:0\", shape=(None, 1455, 1), dtype=float32), but it was called on an input with incompatible shape (None, 4, 1).\n",
      "23/37 [=================>............] - ETA: 0s - loss: 1.6960 - accuracy: 0.5285WARNING:tensorflow:Model was constructed with shape (None, 1455, 1) for input Tensor(\"input_49:0\", shape=(None, 1455, 1), dtype=float32), but it was called on an input with incompatible shape (None, 4, 1).\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.4563 - accuracy: 0.5636 - val_loss: 0.8805 - val_accuracy: 0.5361\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7974 - accuracy: 0.6100 - val_loss: 0.7211 - val_accuracy: 0.5533\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7119 - accuracy: 0.6271 - val_loss: 0.7079 - val_accuracy: 0.5326\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6610 - accuracy: 0.6469 - val_loss: 0.6912 - val_accuracy: 0.5601\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6472 - accuracy: 0.6607 - val_loss: 0.6827 - val_accuracy: 0.5498\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6438 - accuracy: 0.6529 - val_loss: 0.6832 - val_accuracy: 0.5704\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6215 - accuracy: 0.6718 - val_loss: 0.6787 - val_accuracy: 0.6082\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6225 - accuracy: 0.6624 - val_loss: 0.7029 - val_accuracy: 0.5533\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6298 - accuracy: 0.6572 - val_loss: 0.6822 - val_accuracy: 0.6117\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.6744 - val_loss: 0.7229 - val_accuracy: 0.5842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[40],\n",
       "       [13],\n",
       "       [12],\n",
       "       [10]], dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sophomore Course Sequence\n",
    "student_ids = df.alt_id.unique()\n",
    "course_sequences = []\n",
    "for s_id in student_ids:\n",
    "    frame = df[df['alt_id'] == s_id]\n",
    "    course_ids = ' '.join(frame.course_title.str.replace(' ', '').to_list())\n",
    "    course_sequences.append(course_ids)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(course_sequences)\n",
    "encoded_sequences = tokenizer.texts_to_sequences(course_sequences)\n",
    "padded_course_sequences = pad_sequences(encoded_sequences, maxlen=16, dtype='int32', padding='post',\n",
    "truncating='post', value=0.0)\n",
    "feature_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "feature_ids = list(tokenizer.word_index.values())\n",
    "feature_encoder.fit(np.array(feature_ids).reshape(-1,1))\n",
    "feature_encoder.transform(np.array(padded_course_sequences[0]).reshape(-1,1)).reshape(-1,1)\n",
    "padded_reshaped_courses = padded_course_sequences.reshape(1455, 16, 1)\n",
    "\n",
    "model = Sequential()\n",
    "input_layer = Input(shape=(1455, 1))\n",
    "sequential_1 = LSTM(64, activation='relu')\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "model.add(input_layer)\n",
    "model.add(sequential_1)\n",
    "model.add(output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(padded_reshaped_courses, np.array(target), epochs=10, validation_split = .2)\n",
    "\n",
    "\n",
    "pd.DataFrame(feature_encoder.transform(np.array(padded_course_sequences[0]).reshape(-1,1)).todense())\n",
    "np.array(padded_course_sequences[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
